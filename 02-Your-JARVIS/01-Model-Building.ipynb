{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Own JARVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import whisper\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import pyaudio\n",
    "import wave\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Whisper model\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio from the microphone\n",
    "def record_audio(filename, duration, fs=44100, channels=1):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=channels,\n",
    "                    rate=fs,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=1024)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(0, int(fs / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    \n",
    "    print(\"Recording finished\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wave_file = wave.open(filename, 'wb')\n",
    "    wave_file.setnchannels(channels)\n",
    "    wave_file.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wave_file.setframerate(fs)\n",
    "    wave_file.writeframes(b''.join(frames))\n",
    "    wave_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your audio file\n",
    "recorded_audio_path = \"D:/Generative-AI/GenAI-Demo-Projects/02-Your-JARVIS/recorded_audio.wav\"\n",
    "\n",
    "# Path to save the response audio\n",
    "response_audio_path = \"D:/Generative-AI/GenAI-Demo-Projects/02-Your-JARVIS/response.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert speech to text using Whisper\n",
    "def speech_to_text(audio_path):\n",
    "    result = model.transcribe(recorded_audio_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response from GPT-\n",
    "def get_gpt4_response(prompt):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant, respond in short to the user questions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    #return response['choices'][0]['message']['content'].strip()\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to speech\n",
    "def text_to_speech(text, output_path):\n",
    "    tts = gTTS(text)\n",
    "    tts.save(output_path)\n",
    "    sound = AudioSegment.from_file(output_path)\n",
    "    play(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished\n"
     ]
    }
   ],
   "source": [
    "# Record audio from the microphone\n",
    "record_duration = 9  # seconds\n",
    "record_audio(recorded_audio_path, record_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting speech to text...\n",
      "Recognized text:  Hello, I am Harshat Bavar, can you introduce yourself?\n"
     ]
    }
   ],
   "source": [
    "# Convert speech to text\n",
    "print(\"Converting speech to text...\")\n",
    "prompt = speech_to_text(recorded_audio_path)\n",
    "print(f\"Recognized text: {prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting response text to speech...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get response from GPT-4\n",
    "print(\"Getting response from GPT-4...\")\n",
    "response = get_gpt4_response(prompt, api_key)\n",
    "print(f\"GPT-4 response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert response text to speech\n",
    "print(\"Converting response text to speech...\")\n",
    "text_to_speech(response, response_audio_path)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
